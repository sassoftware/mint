#!/usr/bin/python

import base64
import md5
import os
import sha
import time

from conary import dbstore
from conary.conarycfg import ConfigFile
from conary.lib import sha1helper
from conary.lib.cfg import CfgBool, CfgInt

from mint import config
from mint import scriptlibrary
from mint import urltypes

from es3 import s3obj
from es3 import transport

class NoConfigFile(Exception):
    def __init__(self, path = ""):
        self._path = path

    def __str__(self):
        return "Unable to access configuration file: %s" % self._path

class AmazonS3Config(ConfigFile):
    # this file, obviously
    filename                    = 'amazon-s3.conf'

    # how many bytes to upload in one run (default is 10 GB)
    # this better be larger than maxImageSize
    maxMBytesToTx                = (CfgInt, 10240)

    # unpublished builds have to be a week old before consideration for S3
    unpublishedThresholdDays    = (CfgInt, 7)

    # max size for images in bytes in megabytes
    # this is 2 GB until S3 fixes their load balancer bug
    maxImageSize                = (CfgInt, 2048)

    # delete local files after transferring
    deleteAfterTx               = (CfgBool, True)

    # AMAZON CREDENTIALS (left blank intentionally; edit conf file)
    publicKey                   = ''
    privateKey                  = ''

    # S3 bucket to use (the default is here for testing purposes only)
    bucket                      = 'rpath-test'

    # use SSL for S3 operations?
    isSecure                    = (CfgBool, True)

    # how many times should we try to verify the file before moving on?
    maxVerifyAttempts           = (CfgInt, 10)

class AmazonS3Processor(scriptlibrary.SingletonScript):

    # setup logging
    cfg = config.MintConfig()
    cfg.read(config.RBUILDER_CONFIG)
    logPath = os.path.join(cfg.dataPath, 'logs', 'amazon-s3-processor.log')
    del cfg

    def _hashIt(self, path):
        m = md5.new()
        s = sha.new()
        fd = os.open(path, os.O_RDONLY)
        buf = os.read(fd, 40960)
        while len(buf):
            m.update(buf)
            s.update(buf)
            buf = os.read(fd, 40960)
        os.close(fd)
        return (m.hexdigest(), s.hexdigest())

    def _getWorkToDo(self, db):
        cu = db.cursor()

        # determine adds
        adds = []
        earlierThan = time.time() - (self.s3cfg.unpublishedThresholdDays * 86400)
        cu.execute("""SELECT b.buildId, bf.fileId, bf.sha1, fu.url
                        FROM projects p LEFT OUTER JOIN builds b USING (projectId)
                                        LEFT OUTER JOIN publishedReleases pr USING (pubreleaseId)
                                        LEFT OUTER JOIN buildFiles bf USING (buildId)
                                        LEFT OUTER JOIN buildFilesUrlsMap USING (fileId)
                                        LEFT OUTER JOIN filesUrls fu USING (urlId)
                        WHERE (pr.timePublished IS NOT NULL OR (b.timeUpdated < ? OR b.timeCreated IS NULL))
                            AND fu.urltype = ?
                            AND p.hidden = 0""", earlierThan, urltypes.LOCAL)
        rs = cu.fetchall()

        candidateCount = len(rs)
        self.log.info("%d file images are candidates for uploading" % candidateCount)
        self.log.info("restricting to %d MB total (max %d MB per image, min age for unpublished images = %d days)" % (self.s3cfg.maxMBytesToTx, self.s3cfg.maxImageSize, self.s3cfg.unpublishedThresholdDays))

        totalBytesToTx = long(0)
        for currCount, r in enumerate(rs):
            buildId, buildFileId, buildFileSha1, buildFileName = r

            self.log.info("Processing candidate %d..." % (currCount+1, ))
            # screen images that are too big
            try:
                fileSize = os.stat(buildFileName)[6]
                if fileSize == 0:
                    self.log.warning("File %s (buildId=%d) is 0 bytes long; skipping" % (buildFileName, buildId))
                    continue
                elif fileSize > long(self.s3cfg.maxImageSize * 1024 * 1024):
                    self.log.info("File %s (buildId=%d) exceeds maximum image size; skipping" % (buildFileName, buildId))
                    continue

            except (OSError, IOError):
                self.log.warning("File %s (buildId=%d) cannot be read; skipping" % (buildFileName, buildId))
                continue

            adds.append((buildId, buildFileId, buildFileName, fileSize, buildFileSha1))
            self.log.info("Candidate %s added (%d bytes)" % (buildFileName, fileSize))

            totalBytesToTx += fileSize
            if totalBytesToTx > long(self.s3cfg.maxMBytesToTx * 1024 * 1024):
                self.log.info("Maximum bytes to send reached (%d bytes to send)" % totalBytesToTx)
                break

        # determine removes
        removes = []
        cu.execute("""SELECT fu.urlId, fu.urlType, fu.url
                      FROM FilesUrls fu LEFT OUTER JOIN BuildFilesUrlsMap bfum
                         USING (urlId)
                      WHERE (bfum.fileId IS NULL AND fu.urlType in (?, ?))""",
                      urltypes.AMAZONS3, urltypes.AMAZONS3TORRENT)
        rs = cu.fetchall()
        self.log.info("%d file urls are candidates for removal" % len(rs))
        for urlId, urlType, url in rs:
            removes.append((urlId, urlType, url))
            self.log.info("S3 URL is a candidate for deletion: %s" % url)

        return adds, removes

    def _mangleFileName(self, origPath):
        return '/'.join(origPath.split('/')[-3:])

    def _processAdds(self, db, adds):
        uploadCount = len(adds)
        for currCount, addedFileInfo in enumerate(adds):
            self.log.info("Uploading file %s (%d of %d)..." % (addedFileInfo[2], (currCount+1), uploadCount))
            self._addImage(db, addedFileInfo)

    def _processRemoves(self, db, removes):
        for removedFileInfo in removes:
            self._removeImage(db, removedFileInfo)

    def _addImage(self, db, addedFileInfo):

        buildId, fileId, filePath, fileSize, expectedSha1 = addedFileInfo

        try:
            actualFileName = os.path.basename(filePath)
            s3KeyName = self._mangleFileName(filePath)

            self.log.info("Hashing file...")
            (fileMd5, fileSha1) = self._hashIt(filePath)
            self.log.info("MD5=%s, SHA1=%s" % (fileMd5, fileSha1))
            if expectedSha1 and expectedSha1 != fileSha1:
                self.log.warning("Calculated SHA1 does not match DB value; something is wrong. Skipping file!")
                return

            # connect to Amazon S3
            s3auth = transport.S3Auth(self.s3cfg.publicKey,
                    self.s3cfg.privateKey)
            s3conn = transport.S3Connection(s3auth,
                    isSecure=self.s3cfg.isSecure)

            so = s3obj.S3Object(s3conn, self.s3cfg.bucket, s3KeyName)

            # Add some metadata
            so.metadata['file-creator'] = 'rBuilder Online'
            so.metadata['file-sha1'] = fileSha1
            so.metadata['rbuilder-filename'] = actualFileName
            so.metadata['rbuilder-file-id'] = str(fileId)
            so.metadata['rbuilder-build-id'] = str(buildId)
            so.metadata['rbuilder-hostname'] = filePath.split('/')[-3]

            # Add headers
            headers = { 'x-amz-acl': 'public-read',
                        'Content-Disposition': 'attachment; filename=%s' % actualFileName,
                        'Content-MD5': base64.b64encode(sha1helper.md5FromString(fileMd5)) }

            # transfer the file to Amazon S3
            starttime = time.time()
            self.log.info("Sending...")
            resp = so.putFile(filePath, headers)
            endtime = time.time()
            txrate = float((fileSize / (endtime - starttime)) / 1024.0)

            cu = db.transaction()
            if resp.OK() and resp.http_response.status == 200:
                self.log.info("Upload complete (bucket=%s, key=%s, %.1f Kbytes/sec)" % (self.s3cfg.bucket, s3KeyName, txrate))
                cu.execute("UPDATE BuildFiles SET filename = NULL, size = ?, sha1 = ? WHERE fileId = ?", fileSize, fileSha1, fileId)
                cu.execute("INSERT INTO FilesUrls SET urlType = ?, url = ?", urltypes.AMAZONS3, so.getURL())
                urlId = cu.lastrowid
                cu.execute("INSERT INTO BuildFilesUrlsMap SET fileId = ?, urlId = ?", fileId, urlId)
                cu.execute("INSERT INTO FilesUrls SET urlType = ?, url = ?", urltypes.AMAZONS3TORRENT, so.getTorrentURL())
                urlId = cu.lastrowid
                cu.execute("INSERT INTO BuildFilesUrlsMap SET fileId = ?, urlId = ?", fileId, urlId)
                cu.execute("SELECT fu.urlId FROM BuildFilesUrlsMap bfum JOIN FilesUrls fu USING (urlId) WHERE bfum.fileId = ? AND fu.urltype = ?",
                        fileId, urltypes.LOCAL)
                r = cu.fetchall()
                if len(r):
                    localUrlId = r[0][0]
                    cu.execute("DELETE FROM FilesUrls WHERE urlId = ?", localUrlId)
                db.commit()
                self.log.info("Database updated")

                if self.s3cfg.deleteAfterTx:
                    os.unlink(filePath)
                    self.log.info("Deleted local file %s" % filePath)
            else:
                self.log.error("Upload failed (bucket=%s, key=%s, reason=%s)" % (self.s3cfg.bucket, s3KeyName, resp.http_response.reason))
                self.log.error("[HTTP response]: %s" % resp.body)
                db.rollback()
        except:
            raise


    def _removeImage(self, db, removedFileInfo):

        def __removeFromDb(db, removedFileInfo):
            cu = db.transaction()
            urlId, urlType, url = removedFileInfo
            try:
                cu.execute("DELETE FROM FilesUrls WHERE urlId = ?", urlId)
            except:
                db.rollback()
                self.log.info("Failed to remove %s from our database" % url)
                raise

            db.commit()
            self.log.info("Removed %s from our database" % url)


        # connect to Amazon S3
        s3auth = transport.S3Auth(self.s3cfg.publicKey,
                self.s3cfg.privateKey)
        s3conn = transport.S3Connection(s3auth,
                isSecure=self.s3cfg.isSecure)

        urlId, urlType, url = removedFileInfo
        if urlType == urltypes.AMAZONS3TORRENT:
            __removeFromDb(db, removedFileInfo)
        elif urlType == urltypes.AMAZONS3:
            bucket = url.split('/')[-2]
            keyname = url.split('/')[-1]
            self.log.info("Removing %s from Amazon S3 (bucket=%s, key=%s)" % (url, bucket, keyname) )
            try:
                so = s3obj.S3Object(s3conn, bucket, keyname)
                headResponse = so.head()
                if headResponse.OK():
                    deleteResponse = so.delete()
                    if deleteResponse.OK():
                        self.log.info("%s has been removed from Amazon S3" % url)
                        __removeFromDb(db, removedFileInfo)
                    else:
                        self.log.error("%s was not removed from Amazon S3" % url)
                else:
                    self.log.error("%s was in our database, but not on Amazon S3, cleaning up" % url)
                    __removeFromDb(db, removedFileInfo)
            except:
                raise
        else:
            self.log.error("Bad urltype given (%d) for %s" % (urlType, url))

    def __init__(self, aLockpath = scriptlibrary.DEFAULT_LOCKPATH):
        scriptlibrary.SingletonScript.__init__(self, aLockpath)

        self.cfg = config.MintConfig()
        self.cfg.read(config.RBUILDER_CONFIG)
        self.timeStarted = time.time()

        # load local config
        self.s3cfg = AmazonS3Config()
        s3cfgFile = os.path.join(self.cfg.dataPath, self.s3cfg.filename)
        if os.access(s3cfgFile, os.R_OK):
            self.s3cfg.read(s3cfgFile)
        else:
            # write defaults
            try:
                if not os.path.exists(s3cfgFile):
                    f = file(s3cfgFile, 'w+')
                    self.s3cfg.display(out=f)
                    f.close()
            except Exception, e:
                raise NoConfigFile(s3cfgFile)

    def action(self):
        self.log.info('Amazon S3 Processing Script Started')

        try:
            # connect to db
            db = dbstore.connect(self.cfg.dbPath, driver = self.cfg.dbDriver)
            db.loadSchema()

            adds, removes = self._getWorkToDo(db)
            self._processRemoves(db, removes)
            self._processAdds(db, adds)

        except Exception, e:
            self.log.error('Exception occurred in script: %s' % str(e))
            exitcode = 1
        else:
            exitcode = 0

        return exitcode

    def cleanup(self):
        timeTaken = time.time() - self.timeStarted
        self.log.info('Amazon S3 Processing Script Completed (%d seconds)' % timeTaken)


if __name__ == "__main__":
    asp = AmazonS3Processor()
    os._exit(asp.run())

